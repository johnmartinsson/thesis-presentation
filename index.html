<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/custom.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## Bird Species Identification using Convolutional Neural Networks

            Note:

            - Text: Hello and welcome to this presentation on bird species
            identification using convolutional neural networks. My name is John
            Martinsson and I have conducted this study together with my
            supervisor Alexander Schliep.
          </script>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## Summary

            - Introduction
            - Problem Formulation
            - Data set
            - Baseline
            - Goal & Questions
            - Methods
            - Results
            - Conclusions

            Note:
              - Introduction
                - What we are trying to achieve
                - Why it is useful
              - Text: Lets first go through what this presentation will contain.
              First we will go through a short introduction to understand what
              the problem is, and why it is important to solve it. Then we will
              formulate the problem more specifically, look at the data set
              which will be used, and go through the baseline which our results are
              compared against. We will then look at the goal and questions for
              the thesis, the methods used to achieve these goals, and the
              results for the methods. We will end with a short conclusion.
          </script>
        </section>
        <section>
          <h2> What are we trying to achieve? </h2>
          <ul>
            <li class="fragment">
              Identify the most prominently singing bird in an audio recording
            </li>
            <li class="fragment" data-audio-src="audio/bird1.ogg"> Listen to birds </li>
            <li class="fragment" data-audio-src="audio/bird2.ogg"> Bird 1 </li>
            <li class="fragment" data-audio-src="audio/bird3.ogg"> Bird 2 </li>
            <li class="fragment"> Bird 3 </li>
          </ul>
          <aside class="notes">
            <ul>
              <li> Text: We want to be able to identify the most prominently
                singing bird in an audio recording. So given a recording of a
                bird singing in the wild, we would like a classifier to be able
                to output which bird species is singing. In order to get a
                better understanding of why this can be solved using sound we
                will now listen to three different bird species singing... Each
                of the birds have a very characteristic song, and since a human
                can learn to identify them on song, it is reasonable to think
                that it can be done with a machine as well.
            </ul>
          </aside>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## Why birds?
            - Birds are easy to observe <!-- .element: class="fragment" -->
            - Indicators of ecosystem health and changes <!-- .element: class="fragment" -->
            - Conservation efforts <!-- .element: class="fragment" -->
            - Bird population health <!-- .element: class="fragment" -->

            Note:
            - Why is it important to solve the problem?
              - Easy to observe: has made them a standard indicator of ecosystem
              health and changes.
              - Birds are important ecological indicators. Changes in bird populations tell a story about the health of their
              environment.
              - Incline and decline in populations tell a story about what
              envionments are at risk and needs conservation.
              - Knowing which birds and environments are at risk allows informed
              decisions for conservation efforts
              - The health of bird populations is important because they
                - Disperse seeds
                - Pollinate plants
                - Pest control
          </script>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## How is this currently solved?
          </script>
        </section>
        <section>
          <h3>Banding </h3>
          <a title="By Júlio Reis (User:Tintazul) (Original File) [CC BY-SA 2.5 (http://creativecommons.org/licenses/by-sa/2.5)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ARinging_04_Ringing.jpg"><img width="512" alt="Ringing 04 Ringing" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Ringing_04_Ringing.jpg/512px-Ringing_04_Ringing.jpg"/></a>
          <aside class="notes">
            <p>
              One is banding of birds where small metal rings are placed around
              their feet to allow tracking of individuals and bird populations.
              This can also indicate the diversity of birds in an area, and
              the size of the population.
            </p>
          </aside>
        </section>
        <section>
          <h3> Manual Point Count </h3>
          <img src="images/GuyPointCount.svg" height="80%" width ="80%">
          <aside class="notes">
            <ul>
              <li> Requires expertise </li>
              <li> Time consuming </li>
              <li> Biased by the observer </li>
              <li> Consistency is a necessity </li>
            </ul>
          </aside>
        </section>
        <section>
          <h3> Point Count Data </h3>
          <img src="images/PointCount.svg" height="40%" width ="40%">
          <aside class="notes">
            <p> Example of a data point collected during a point count. With
            many such counts it is possible to get a good understanding of the
            bird population in the surveyed area.</p>
          </aside>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## How could this be solved?
          </script>
        </section>
        <section>
          <h3> Automatic Point Count </h3>
          <img src="images/GuyPointCountMic.svg" height="80%" width ="80%">
          <aside class="notes">
            <ul>
              <li>Give man a microphone, and bird classification system</li>
              <li>Automatic point counts </li>
              <li>Predict relative position of bird</li>
              <li>Predict the species of the singing bird</li>
              <li> Requires no expertise </li>
              <li> Consistent and less biased </li>
              <li> Can be paced for continuous monitoring </li>
            </ul>
          </aside>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Problem formulation
            </script>
          </section>
          <section>
            <h2> Signal Classification </h2>
            <div>
              \[ X = \{(y_i, x_i)\}, i=1, \dots, N \]
            </div>
            <div class="equation">
            \[ w = \arg\min_{w} \sum_{(y, x) \in X} loss(f(x), f_w(x)) \]
            </div>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <ul>
              <li> Reduce the size of the input </li>
              <li> Trade-off between separability and contraction </li>
            </ul>
            <div class="equation">
              \[ \Phi(x) \neq \Phi(x') \text{ if } f(x) \neq f(x') \]
            </div>
            <aside class="notes">
              <ul>
                <li> Enable efficient training </li>
                <li> Raw audio data not good </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <div>
              \[
              \begin{aligned}
              \text{if } &amp; \Phi((x, y)) = x \text{ and } f((x, y)) = y \\
              \text{ then } &amp; f((3, 1)) \neq f((3, 2)) \\
              \text{ but } &amp; \Phi((3, 1)) = \Phi((3, 2)) \\
              \end{aligned}
              \]
            </div>
          </section>
        </section>
        <section data-markdown>
          <script type="text/template">
            ## Data set
            - BirdCLEF 2016
            - 24,600 training files
            - 8,600 test files
            - 999 different bird species
            - Field recordings
            - Can contain background species
          </script>
        </section>
        <section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Baseline
              - Preprocessing <!-- .element: class="fragment" -->
              - Data augmentation <!-- .element: class="fragment" -->
              - Feature extraction <!-- .element: class="fragment" -->
              - Signal classification <!-- .element: class="fragment" -->

              Note:
              - Lets look at the baseline which we want to improve upon
            </script>
          </section>
          <section>
            <img src="images/sprengel_binary_mask.png" height="50%" width ="50%">
            <aside class="notes">
              <ul>
                <li> Find bird vocals </li>
                <li> Compute amplitude spectrogram of recording </li>
                <li> Find parts with high energy using a median clipping </li>
                <li> Apply an erosion filter to filter out the regions with
                  most energy </li>
                <li> Perform a dilation filter to highlight these regions </li>
                <li> Columns with at least one non-zero value (black pixel) are
                  marked as bird vocals, or signal </li>
                <li> Other parts are marked as noise </li>
              </ul>
            </aside>
          </section>
          <section>
            <h4> Separate Signal and Noise </h4>
            <img src="images/noise_signal.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> We use this to extract the signal and noise part of the
                  audio recording </li>
              </ul>
            </aside>
          </section>
          <section data-markdown>
            <script type="text/template">
              ## Data Augmentation
            </script>
          </section>
          <section>
            <h4> Data Augmentation: Same Class</h4>
            <img src="images/same_class_augmentation.png" height="80%" width ="80%">
            <aside class="notes">
              <p> We randomly choose two segments from the same class and then
              add them together. We weigh the signals using a randomly chosen
              alpha value. </p>
            </aside>
          </section>
          <section>
            <h4> Data Augmentation: Noise </h4>
            <img src="images/noise_augmentation.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Add three noise segments </li>
                <li> Harder to classify </li>
                <li> Noise invariant, Generalization </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <img src="images/logspectrogram.png" height="80%" width="80%">
            <aside class="notes">
              <ul>
                <li> Data augmentation in time domain </li>
                <li> Time-spectral representation </li>
              </ul>
            </aside>
          </section>
          <section>
            <h3> Convolutional Neural Network </h3>
						<a title="By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons"
               href="https://commons.wikimedia.org/wiki/File%3ATypical_cnn.png">
              <img width="100%" alt="Typical cnn" src="images/typical_cnn.png"/>
            </a>
            <aside class="notes">
              <ul>
                <li> Convolving layers learns to extract relevant features from
                  the input </li>
                <li> Pooling layers reduces the size of the input </li>
                <li> Batch normalization reduces the risk of vanishing or
                  exploding gradients </li>
                <li> The final feature maps are used as input to a fully
                  connected neural network which is the part that actually
                  learns to classify the audio recording </li>
                <li> The fully connected network has a dropout rate of 40% to
                  improve generalization </li>
              </ul>
            </aside>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Convolutional neural network
              - 5x BatchNormalization, Convolution2D, MaxPooling2D
              - BatchNormalization
              - Flatten
              - Dropout (40%)
              - Dense (1024 neurons)
              - Dropout (40%)
              - Dense (999 neurons)

              Note:
              - Bach normalization reduces the risk of vanishing/exploding
              gradients/signals
              - Convolving layers learns feature extraction mappings
              - Max pooling reduces the size of the features
              - Fully connected network learns to classify the extracted
              features.
            </script>
          </section>
        </section>
        <section>
          <h2> Goal of the Thesis </h2>
          <ul>
            <li class="fragment"> Goal: Improve upon the state-of-the-art baseline</li>
            <li class="fragment">
              New convolutional neural network architecture
            </li>
            <li class="fragment">
              New data augmentation technique
            </li>
            <li class="fragment">
              Meta-data fusion
            </li>
            <li class="fragment">
              Question: Can these techniques be used to improve classification accuracy?
            </li>
          </ul>
          <aside class="notes">
            <ul>
              <li> We have looked at a new convolutional neural network
                architecture called deep residual neural networks, a new data
                augmentation technique called multiple-width frequency-delta
                augmentaion, and a way of using other information, in addition
                to the audio recording, to classify the sining bird. </li>
              <li> Answered by training on a large data set </li>
              <li> Evaluating on a large test set </li>
              <li> Compare results with baseline </li>
            </ul>
          </aside>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Methods
            </script>
          </section>
          <section>
            <h2> Network architecture: deep residual neural networks </h2>
            <ul>
              <li> Introduce shortcuts </li>
              <li> Allow training of very deep networks </li>
            </ul>
            <aside class="notes">
              <ul>
                <li> Shortcuts allow signal to flow more easily through the network </li>
                <li> Why? - deeper is better </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Shortcut </h2>
            <img src="images/residual-function.png" height="50%" width ="50%">
            <div class="fragment">
              \[ y = \mathcal{H}(x) = \mathcal{F}(x) + x \]
            </div>
            <aside class="notes">
              <ul>
                <li> Reason: optimal mapping are usually closer to the identity
                  mapping, than the zero mapping, meaning that it is easier to
                  learn the difference between identity mapping and the optimal
                mapping, than the zero mapping and the optimal mapping </li>
                <li> Why? - assumed easier to learn the residual mapping </li>
              </ul>
            </aside>
          </section>
          <section>
            <h4> Data augmentation: Multiple-width frequency-delta </h4>
            <img src="images/mwfd.png" width ="100%">
            <p style="font-size:30%"> Image source: Yoonchang Han and Kyogu Lee. Acoustic scene classification using convolutional neural network and multiple-width frequency-delta data augmentation. 14(8):1–11, 2016 </p>
            <aside class="notes">
              <ul>
                <li> Mel-frequency cepstral coefficients (MFCCs) </li>
                <li> Compact feature representation </li>
                <li> Deltas of MFCCs </li>
                <li> Delta gives dynamic information, trajectory </li>
                <li> First order frame-to-frame difference </li>
                <li> Width determines the number of frames used to compute the
                  deltas </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Meta-data fusion </h2>
            <div>
              \[
              Pr(bird_j|elevation, song) = \\
              \frac{Pr(bird_j|song)Pr(elevation|bird_j)}
              {\sum_{i=1}^n Pr(bird_i|song)Pr(elevation|bird_i)}
              \]
            </div>
            <aside class="notes">
              <ul>
                <li> Assumption: Pr(elevation|song,bird) = Pr(elevation|bird) </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Pseudo probability </h2>
            <div>
            \[Pr(bird_j | song) \]
            </div>
						<a title="By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons"
               href="https://commons.wikimedia.org/wiki/File%3ATypical_cnn.png">
              <img width="100%" alt="Typical cnn" src="images/typical_cnn.png"/>
            </a>
            <aside class="notes">
              <ul>
                <li>
                Computed by convolutional neural network. Output of the last 999
                neurons (normalized to sum to 1)
                </li>
                <li> Pr(bird j | song) is the output of the j:th neuron of the
                  network </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Elevation likelihood </h2>
            <div>
            \[Pr(elevation | bird_j) \]
            </div>
            <img src="images/elevation_histogram.png" height="60%" width ="60%">
          </section>
          <section>
            <h2> Elevation likelihood </h2>
            <div>
            \[Pr(elevation | bird_j) \]
            </div>
            <img src="images/mixture_density.png" height="60%" width ="60%">
            <aside class="notes">
              <ul>
                <li> Gaussian modeled by computing mean and standard deviation
                  of the available elevations for the bird species </li>
                <li> Mixture density of Gaussian and Uniform </li>
                <li> Tending towards the uniform if we have a small amount of
                  elevation data for the species </li>
                <li> Tending towads the Gaussian if the have a large amount of
                  elevation data for the species </li>
              </ul>
            </aside>
          </section>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Results
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ## Deep residual neural network
              | Method | MAP (with) | MAP (without) |
              | --- | --- | --- |
              | Baseline | 55.8% | 69.7% |
              | Resnet 18 | 53.8% | 67.9% |
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Meta-data fusion
              | Method | Coverage error (without fusion) | Coverage error (with fusion) |
              | --- | --- | --- |
              | Baseline | $25.3 \pm 0.3$ | $21.1 \pm 0.3$ |
              | Resnet | $28.7 \pm 1.5$ | $24.7 \pm 1.2$ |
              - Reduce coverage error by ~4 <!-- .element: class="fragment" -->
              - MAP score is barely affected <!-- .element: class="fragment" -->

              Note:
              - Ground truth labels are on average predicted higher
              - Coverage error from 25 to 21 (baseline)
              - Coverage error from 28 to 24 (resnet)
              - Not high enough to affect the MAP score
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## What could the problems be?
            </script>
          </section>
          <section>
            <img src="images/descending_accuracy_cuberun.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Accuracy is uneven for different sound classes </li>
              </ul>
            </aside>
          </section>
          <section>
            <img src="images/training_samples_by_number_of_predictions.png"
            height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Relative number of training samples is uneven </li>
              </ul>
            </aside>
          </section>
          <section>
            <img src="images/confusion_matrix_bot100_resnet_18_1.png"
            height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Overfitting to a sound class can be a problem </li>
              </ul>
            </aside>
          </section>
        </section>
        <section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Conclusions
              - Deep residual neural network can learn to classify bird species
              based on bird song
              - The accuracy is close to the state-of-the-art
              - Meta-data fusion can be used to reduce coverage error

              Note:
                - The study shows that deep residual neural networks can learn to
                classify bird species based on bird song and that the mean
                average precision of the classifier nearly matches the
                state-of-the-art.
                - We further develop a proof of concept for meta-data fusion
                which indicates that fusion of elevation data can be used to
                increase the accuracy of the model, and in particular decrease
                its coverage error.
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Way forward
              - Find a way to mitigate the problem of the uneven data set <!-- .element: class="fragment" -->
              - Modify the loss function to also consider background species <!-- .element: class="fragment" -->
              - Tune the hyperparameters of the residual neural networks <!-- .element: class="fragment" -->
            </script>
          </section>
        </section>
			</div>
		</div>
		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
        audio : {
          defaultDuration: 600
        },

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/audio-slideshow/slideshow-recorder.js', condition: function( ) { return !!document.body.classList; } },
          { src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } }
				]
			});
		</script>
	</body>
</html>
