<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/custom.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/solarized.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
          <script type="text/template">
            ## Bird Species Identification using Convolutional Neural Networks
          </script>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## Summary

            - Introduction
            - Problem Formulation
            - Aim and Questions
            - Methods
            - Results
            - Conclusions

            Note:
              - Introduction
                - What we are trying to achieve
                - Why it is useful
          </script>
        </section>
        <section>
          <h2> What are we trying to achieve? </h2>
          <ul>
            <li class="fragment">
              Automatic classification of bird species based on audio recordings
            </li>
            <li class="fragment" data-audio-src="audio/bird1.ogg"> Listen to birds </li>
            <li class="fragment" data-audio-src="audio/bird2.ogg"> Bird 1 </li>
            <li class="fragment" data-audio-src="audio/bird3.ogg"> Bird 2 </li>
            <li class="fragment"> Bird 3 </li>
          </ul>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## Why birds?
            - Birds are easy to observe <!-- .element: class="fragment" -->
            - Indicators of ecosystem health and changes <!-- .element: class="fragment" -->
            - Conservation efforts <!-- .element: class="fragment" -->
            - Bird population health <!-- .element: class="fragment" -->

            Note:
            - Why is it important to solve the problem?
              - Easy to observe: has made them a standard indicator of ecosystem
              health and changes.
              - Birds are important ecological indicators
                - Changes in bird populations tell a story about the health of their
                  environment. You could say that monitoring birds is a way of taking the
                  pulse of nature.
              - Knowing which birds and environments are at risk allows informed
              decisions for conservation efforts
              - The health of bird populations is important because they
                - Disperse seeds
                - Pollinate plants
                - Pest control
          </script>
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## How is this currently solved?
          </script>
        </section>
        <section>
          <h3>Banding </h3>
          <a title="By JÃºlio Reis (User:Tintazul) (Original File) [CC BY-SA 2.5 (http://creativecommons.org/licenses/by-sa/2.5)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ARinging_04_Ringing.jpg"><img width="512" alt="Ringing 04 Ringing" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Ringing_04_Ringing.jpg/512px-Ringing_04_Ringing.jpg"/></a>
          <aside class="notes">
            <p>
              Banding is done by marking birds to enable tracking of individual
              movements. This is an invasive, but not harmful, monitoring
              technique, because you have to catch the birds in nets in order to
              mark them.
            </p>
          </aside>
        </section>
        <section>
          <h3> Manual Point Count </h3>
          <img src="images/GuyPointCount.svg" height="80%" width ="80%">
          <aside class="notes">
            <ul>
              <li> Requires expertise </li>
              <li> Time consuming </li>
              <li> Biased by the observer </li>
              <li> Consistency is a necessity </li>
            </ul>
          </aside>
        </section>
        <section>
          <h3> Point Count Data </h3>
          <img src="images/PointCount.svg" height="40%" width ="40%">
        </section>
				<section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
          <script type="text/template">
            ## How could this be solved?
          </script>
        </section>
        <section>
          <h3> Automatic Point Count </h3>
          <img src="images/GuyPointCountMic.svg" height="80%" width ="80%">
          <aside class="notes">
            <ul>
              <li>Give man a microphone, and bird classification system</li>
              <li>Automatic point counts </li>
              <li>Predict relative position of bird</li>
              <li>Predict the species of the singing bird</li>
              <li> Requires no expertise </li>
              <li> Consistent and less biased </li>
              <li> Can be paced for continuous monitoring </li>
            </ul>
          </aside>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Problem formulation
            </script>
          </section>
          <section>
            <h2> Signal Classification </h2>
            <div>
              \[ X = \{(y_i, x_i)\}, i=1, \dots, N \]
            </div>
            <div class="equation">
            \[ w = \arg\min_{w} \sum_{(y, x) \in X} loss(f(x), f_w(x)) \]
            </div>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <ul>
              <li> Reduce the size of the input </li>
              <li> Trade-off between separability and contraction </li>
            </ul>
            <div class="equation">
              \[ \Phi(x) \neq \Phi(x') \text{ if } f(x) \neq f(x') \]
            </div>
            <aside class="notes">
              <ul>
                <li> Enable efficient training </li>
                <li> Raw audio data not good </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <div>
              \[
              \begin{aligned}
              \text{if } &amp; \Phi((x, y)) = x \text{ and } f((x, y)) = y \\
              \text{ then } &amp; f((3, 1)) \neq f((3, 2)) \\
              \text{ but } &amp; \Phi((3, 1)) = \Phi((3, 2)) \\
              \end{aligned}
              \]
            </div>
          </section>
        </section>
        <section data-markdown>
          <script type="text/template">
            ## Data set
            - BirdCLEF 2016
            - 24,600 training files
            - 8,600 test files
            - 999 different bird species
            - Field recordings
            - Can contain background species
          </script>
        </section>
        <section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Baseline
              - Preprocessing <!-- .element: class="fragment" -->
              - Data augmentation <!-- .element: class="fragment" -->
              - Feature extraction <!-- .element: class="fragment" -->
              - Signal classification <!-- .element: class="fragment" -->

              Note:
              - Lets look at the baseline which we want to improve upon
            </script>
          </section>
          <section>
            <img src="images/sprengel_binary_mask.png" height="50%" width ="50%">
            <aside class="notes">
              <ul>
                <li> Find bird vocals </li>
                <li> Compute amplitude spectrogram of recording </li>
                <li> Find parts with high energy using a median clipping </li>
                <li> Apply an erosion filter to filter out the regions with
                  most energy </li>
                <li> Perform a dilation filter to highlight these regions </li>
                <li> Columns with at least one non-zero value (black pixel) are
                  marked as bird vocals, or signal </li>
                <li> Other parts are marked as noise </li>
              </ul>
            </aside>
          </section>
          <section>
            <h4> Separate Signal and Noise </h4>
            <img src="images/noise_signal.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> We use this to extract the signal and noise part of the
                  audio recording </li>
              </ul>
            </aside>
          </section>
          <section data-markdown>
            <script type="text/template">
              ## Data Augmentation
            </script>
          </section>
          <section>
            <h4> Data Augmentation: Same Class</h4>
            <img src="images/same_class_augmentation.png" height="80%" width ="80%">
          </section>
          <section>
            <h4> Data Augmentation: Noise </h4>
            <img src="images/noise_augmentation.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Add three noise segments </li>
                <li> Harder to classify </li>
                <li> Noise invariant, Generalization </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Feature Extraction </h2>
            <img src="images/logspectrogram.png" height="80%" width="80%">
            <aside class="notes">
              <ul>
                <li> Data augmentation in time domain </li>
                <li> Time-spectral representation </li>
              </ul>
            </aside>
          </section>
          <section>
            <h3> Convolutional Neural Network </h3>
						<a title="By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons"
               href="https://commons.wikimedia.org/wiki/File%3ATypical_cnn.png">
              <img width="100%" alt="Typical cnn" src="images/typical_cnn.png"/>
            </a>
            <aside class="notes">
              <ul>
                <li> Convolving layers learns to extract relevant features from
                  the input </li>
                <li> Pooling layers reduces the size of the input </li>
                <li> Batch normalization reduces the risk of vanishing or
                  exploding gradients </li>
                <li> The final feature maps are used as input to a fully
                  connected neural network which is the part that actually
                  learns to classify the audio recording </li>
                <li> The fully connected network has a dropout rate of 40% to
                  improve generalization </li>
              </ul>
            </aside>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Convolutional neural network
              - 5x BatchNormalization, Convolution2D, MaxPooling2D
              - BatchNormalization
              - Flatten
              - Dropout (40%)
              - Dense (1024 neurons)
              - Dropout (40%)
              - Dense (999 neurons)

              Note:
              - Bach normalization reduces the risk of vanishing/exploding
              gradients/signals
              - Convolving layers learns feature extraction mappings
              - Max pooling reduces the size of the features
              - Fully connected network learns to classify the extracted
              features.
            </script>
          </section>
        </section>
        <section>
          <h2> Goal of the Thesis </h2>
          <ul>
            <li class="fragment"> Goal: Improve upon the state-of-the-art baseline</li>
            <li class="fragment">
              New convolutional neural network architecture
            </li>
            <li class="fragment">
              New data augmentation technique
            </li>
            <li class="fragment">
              Meta-data fusion
            </li>
            <li class="fragment">
              Question: Can these techniques be used to improve classification accuracy?
            </li>
          </ul>
          <aside class="notes">
            <ul>
              <li> We have looked at a new convolutional neural network
                architecture called deep residual neural networks, a new data
                augmentation technique called multiple-width frequency-delta
                augmentaion, and a way of using other information, in addition
                to the audio recording, to classify the sining bird. </li>
              <li> Answered by training on a large data set </li>
              <li> Evaluating on a large test set </li>
              <li> Compare results with baseline </li>
            </ul>
          </aside>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Methods
            </script>
          </section>
          <section>
            <h2> Network architecture: deep residual neural networks </h2>
            <ul>
              <li> Introduce shortcuts </li>
              <li> Allow training of very deep networks </li>
            </ul>
            <aside class="notes">
              <ul>
                <li> Shortcuts allow signal to flow more easily through the network </li>
                <li> Why? - deeper is better </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Shortcut </h2>
            <img src="images/residual-function.png" height="50%" width ="50%">
            <div class="fragment">
              \[ y = \mathcal{H}(x) = \mathcal{F}(x) + x \]
            </div>
            <aside class="notes">
              <ul>
                <li> Reason: optimal mapping are usually closer to the identity
                  mapping, than the zero mapping, meaning that it is easier to
                  learn the difference between identity mapping and the optimal
                mapping, than the zero mapping and the optimal mapping </li>
                <li> Why? - assumed easier to learn the residual mapping </li>
              </ul>
            </aside>
          </section>
          <section>
            <h4> Data augmentation: Multiple-width frequency-delta </h4>
            <img src="images/mwfd.png" width ="100%">
            <p style="font-size:30%"> Image source: Yoonchang Han and Kyogu Lee. Acoustic scene classification using convolutional neural network and multiple-width frequency-delta data augmentation. 14(8):1â11, 2016 </p>
            <aside class="notes">
              <ul>
                <li> Mel-frequency cepstral coefficients (MFCCs) </li>
                <li> Compact feature representation </li>
                <li> Deltas of MFCCs </li>
                <li> Delta gives dynamic information, trajectory </li>
                <li> First order frame-to-frame difference </li>
                <li> Width determines the number of frames used to compute the
                  deltas </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Meta-data fusion </h2>
            <div>
              \[
              Pr(bird_j|elevation, song) = \\
              \frac{Pr(bird_j|song)Pr(elevation|bird_j)}
              {\sum_{i=1}^n Pr(bird_i|song)Pr(elevation|bird_i)}
              \]
            </div>
            <aside class="notes">
              <ul>
                <li> Assumption: Pr(elevation|song,bird) = Pr(elevation|bird) </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Pseudo probability </h2>
            <div>
            \[Pr(bird_j | song) \]
            </div>
						<a title="By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons"
               href="https://commons.wikimedia.org/wiki/File%3ATypical_cnn.png">
              <img width="100%" alt="Typical cnn" src="images/typical_cnn.png"/>
            </a>
            <aside class="notes">
              <ul>
                <li>
                Computed by convolutional neural network. Output of the last 999
                neurons (normalized to sum to 1)
                </li>
                <li> Pr(bird j | song) is the output of the j:th neuron of the
                  network </li>
              </ul>
            </aside>
          </section>
          <section>
            <h2> Elevation likelihood </h2>
            <img src="images/elevation_histogram.png" height="60%" width ="60%">
          </section>
          <section>
            <h2> Elevation likelihood </h2>
            <div>
            \[Pr(elevation | bird_j) \]
            </div>
            <img src="images/mixture_density.png" height="60%" width ="60%">
            <aside class="notes">
              <ul>
                <li> Gaussian modeled by computing mean and standard deviation
                  of the available elevations for the bird species </li>
                <li> Mixture density of Gaussian and Uniform </li>
                <li> Tending towards the uniform if we have a small amount of
                  elevation data for the species </li>
                <li> Tending towads the Gaussian if the have a large amount of
                  elevation data for the species </li>
              </ul>
            </aside>
          </section>
        </section>
        <section>
          <section data-markdown>
            <script type="text/template">
              ## Results
            </script>
          </section>
          <section data-markdown>
            <script type="text/template">
              ## Deep residual neural network
              | Method | MAP (with) | MAP (without) |
              | --- | --- | --- |
              | Baseline | 55.8% | 69.7% |
              | Resnet 18 | 53.8% | 67.9% |
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Meta-data fusion
              | Method | Coverage error (without fusion) | Coverage error (with fusion) |
              | --- | --- | --- |
              | Baseline | $25.3 \pm 0.3$ | $21.1 \pm 0.3$ |
              | Resnet | $28.7 \pm 1.5$ | $24.7 \pm 1.2$ |
              - Reduce coverage error by ~4 <!-- .element: class="fragment" -->
              - MAP score is barely affected <!-- .element: class="fragment" -->

              Note:
              - Ground truth labels are on average predicted higher
              - Coverage error from 25 to 21 (baseline)
              - Coverage error from 28 to 24 (resnet)
              - Not high enough to affect the MAP score
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## What could the problems be?
            </script>
          </section>
          <section>
            <img src="images/descending_accuracy_cuberun.png" height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Accuracy is uneven for different sound classes </li>
              </ul>
            </aside>
          </section>
          <section>
            <img src="images/training_samples_by_number_of_predictions.png"
            height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Relative number of training samples is uneven </li>
              </ul>
            </aside>
          </section>
          <section>
            <img src="images/confusion_matrix_bot100_resnet_18_1.png"
            height="80%" width ="80%">
            <aside class="notes">
              <ul>
                <li> Overfitting to a sound class can be a problem </li>
              </ul>
            </aside>
          </section>
        </section>
        <section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Conclusions
              - Deep residual neural network can learn to classify bird species
              based on bird song
              - The accuracy is close to the state-of-the-art
              - Meta-data fusion can be used to reduce coverage error

              Note:
                - The study shows that deep residual neural networks can learn to
                classify bird species based on bird song and that the mean
                average precision of the classifier nearly matches the
                state-of-the-art.
                - We further develop a proof of concept for meta-data fusion
                which indicates that fusion of elevation data can be used to
                increase the accuracy of the model, and in particular decrease
                its coverage error.
            </script>
          </section>
          <section data-markdown data-separator="^\n\n\n" data-separator-vertical="^\n\n" data-separator-notes="^Note:">
            <script type="text/template">
              ## Way forward
              - Find a way to mitigate the problem of the uneven data set <!-- .element: class="fragment" -->
              - Modify the loss function to also consider background species <!-- .element: class="fragment" -->
              - Tune the hyperparameters of the residual neural networks <!-- .element: class="fragment" -->
            </script>
          </section>
        </section>
			</div>
		</div>
		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>
		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
        audio : {
          defaultDuration: 600
        },

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/menu/menu.js' },
					{ src: 'plugin/math/math.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/audio-slideshow/slideshow-recorder.js', condition: function( ) { return !!document.body.classList; } },
          { src: 'plugin/audio-slideshow/audio-slideshow.js', condition: function( ) { return !!document.body.classList; } }
				]
			});
		</script>
	</body>
</html>
